<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>Density Functions</title>
    <link rel="stylesheet" href="tufte.min.css"/>
    <style>
      .katex { font-size: 0.95em !important; }
      foreignObject { font-size: 2em; }
    </style>
    <!-- local D3 -->
    <script type="text/javascript" src="d3.min.js" charset="utf-8"></script>
    <!-- fetch D3 -->
    <!-- <script src="https://d3js.org/d3.v5.min.js"></script> -->

    <!-- local KaTeX -->
    <link defer rel="stylesheet" href="katex.min.css">
    <script defer src="katex.min.js" charset="uft-8"></script>
    <script defer src="auto-render.min.js" charset="utf-8" onload="renderMathInElement(document.body);"></script>
    <!-- fetch KaTeX-->
    <!--   <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> -->
    <!-- <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> -->
    <!-- <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script> -->

    <!-- local jStat -->
    <script type="text/javascript" src="jstat.min.js"></script>
    <!-- fetch jStat -->
    <!-- <script src="https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js"></script> -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <article>
      <h1 id="page">Density Functions</h1>
      <p class="subtitle">Edward A. Roualdes</p>
      <section>
        <h2 id="contents">Contents</h2>
        <bold><a href="#introduction">Introduction</a></bold></br>
        <bold><a href="#discrete">Discrete Sample Spaces</a></bold></br>
        <bold><a href="#continuous">Continuous Sample Spaces</a></bold></br>
        <bold><a href="#uses">Uses</a></bold></br>
      </section>

      <section>
        <h2 id="introduction">Introduction</h2>
        <p>
          Density functions describe which sets of numbers are more
          likely, and more specifically, exactly how likely any given
          set of numbers is.  These functions assign density, hence
          the name, to specific numbers, which is an analogous concept
          to probability, but is not bounded above by \( 1 \).  The
          higher the density, the more likely these numbers.  The
          lower the density, the less likely these numbers.
        </p>

        <p>
          Imagine you want to assign probabilities to the set
          consisting of \( 0 \) and \( 1 \), namely \( S = \{0, 1\}
          \).  You could use a density function to say that \( 0 \) is
          more likely than \( 1 \), and specifically \( 0 \) should be
          assigned the density \( 0.75 \). Because the space \( S \)
          is countable, a peculiarity of discrete density functions
          forces the number \( 1 \) to have density equal to \( 0.25
          \).<span class="marginnote">This peculiarity does not hold
          for density functions defined on uncountable spaces.</span>
        </p>

        <p>
          Or maybe you're interested in assigning probabilities to the
          set of positive real numbers, \( S = \mathbb{R}^+ \).  And
          say the interval \( [0, 1) \) is more likely, and the
          interval \( [1, \infty) \) is less likely.  Density
          functions are a class of functions that provide structure to
          assigning probability to sets of
          numbers.<span class="marginnote">Technically, density
          functions assign probability to some subsets of \( S \).
          The better understand the qualifier some,
          see <a href="https://stats.libretexts.org/Bookshelves/Probability_Theory/Book%3A_Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/02%3A_Probability_Spaces" target="_blank" rel="noopener">Chapter
          2: Probability Spaces</a> in Kyle Siegrist's <a href="https://en.wikipedia.org/wiki/Open_educational_resources" target="_blank" rel="noopener">OER</a> book
          "Probability, Mathematical Statistics, and Stochastic
          Processes."</span>
        </p>

        <p>
          Density functions are inherently tied to distribution
          functions, but most statisticians tend to think in terms of
          the former.  It's in fact proper, albeit cumbersome, to
          refer to a density function of a specific, named,
          distribution.  Throughout these notes we will refer to
          density functions in this more cumbersome way for
          clarity.<span class="marginnote">There is one main
          connection between a density function and a distribution
          function, though the connection is viewed in two directions.
          For details on the primary direction, see the lecture notes
          TODO Distribution Functions, and for the secondary direction
          see the lecture notes on <a href="lecturenotes">Expected
          Value</a>.</span>
        </p>

        <p>
          TODO need also emphasize somewhere that they are families of
          density functions indexed by \( \theta \in \Theta \subset
          \mathbb{R}^D \).
        </p>

        <p>
          Density functions play a prominent role in applied
          statistics, despite only existing in certain scenarios.  For
          now, we'll ignore these special cases and focus on gaining
          an understanding of density functions themselves, instead of
          the technical context surrounding density functions.
        </p>


        <p>
          The easiest way to think about density functions is as a
          class of functions, where the class is defined by only three
          properties.  The properties that make an ordinary function
          \( f \) defined on a space \( S \subset \mathbb{R}^D \) a
          density function are <span class="marginnote">I use the
          phrase density function to refer to both probability mass
          functions and probability density functions.</span>

          <ol>
            <li> \( f \) is real-valued, \( f : S \rightarrow \mathbb{R} \),
              
            <li> \( f \) is non-negative, for any \( x \in S \), \( f(x) \geq 0 \), and </li>
            
            <li> the area under the function is equal to one, \(
            \mathbb{E}_f[\mathbb{I}_S] = 1
            \).<span class="marginnote">We define both the expectation
            operator \( \mathbb{E}_f \) and the indicator function \(
            \mathbb{I}_A \) below.  For more details on the expection
            operator and expected value see the the lecture notes on
            TODO expectation.</span>
              
          </ol>
        </p>

        <p>
          The operator \( \mathbb{E}_f \) is known as the expectation
          operator and depends on the space on which the density
          function is defined.  When the sample space is countable the
          density function is said to be the density function of a
          discrete distribution.  When the sample space is uncountable
          the density function is said to be a density function of a
          continuous distribution.  There's ample misuse of these
          terms, but we'll stick with them anyway, as they are quite
          common.
        </p>

        <p>
          These lecture notes will discuss discrete and then
          continuous distributions, providing examples of more common
          density functions for each.  Then we'll give some common
          uses of density functions, providing references and links to
          more thorough discussions of the particular uses.
        </p>
      </section>

      <section>
          <h2 id="discrete">Discrete Sample Spaces</h2>
          <p>
            Density functions defined on a countable sample space \( S
              \) are said to be density functions of a discrete
              distribution.  When \( S \) is countable, say \( S
              \subset \mathbb{N} \), then the expectation operator \(
              \mathbb{E}_f \) in the third property above becomes a
              sum,<span class="marginnote">The symbol \(
              \mathbb{I}_S(x) \) is an indicator function on the set
              \( S \) that indicates when the argument \( x \) is
              contained within the set \( S \).  When \( x \in S \),
              then \( \mathbb{I}_S(x) = 1 \) and when \(x \not\in S
              \), then \( \mathbb{I}_S(x) = 0 \).</span>

            \[ \mathbb{E}_f[\mathbb{I}_S] = \sum_{x \in \mathbb{N}} \mathbb{I}_S(x) f(x) = 1. \]
          </p>

          <p>
            Becasuse the summand takes on the value \( f(x) \)
            whenever \(x \in S \) and \( 0 \) otherwise, this
            expectation is saying that the area under the density
            function sums to \( 1 \) within the set \( S \).  While
            the notation might be new, this property is not
            unexpected.  You probably already understood that the sum
            of probabilities across all possible values must be one.
            This idea comes from a property of density functions.
          </p>

          <p>
            A unique feature of density functions on countable spaces
            is that probabilities of points are equal to the density
            function evaluated at that point, \( \mathbb{P}[\{x\}] =
            f(x) \).  This property is not true for density
            functions defined on uncountable spaces.
          </p>

          <p>
            The first two properties provide structure to the term
            density, such that we can interpret density functions
            analogous to probability.  The first property says that
            density functions are real-valued, that is we should just
            get out one number.  The second property tells us that we
            will never get negative numbers from density functions,
            even if we input a negative number.
          </p>

          <p>
            <b>Example: Bernoulli
              Distribution</b><span class="marginnote">For more
              details, check out my lecture notes on the family
              of <a href="notes/bernoulli.html">Bernoulli
              Distributions</a>.</span>. The density function
              associated with a Bernoulli distribution is defined on
              two states \( 0 \) and \( 1 \), such that \( S = \{0,
              1\} \).  In this case, we'd call \( f \) the density
              function of a Bernoulli distribution with parameter \( p
              \) and it has the following
              form<span class="marginnote">The extra \( | \, p \)
              notation in \( f(x \, | \, p) \) is used to emphasize
              that this density function depends on the parameter \( p
              \).  Since it is common to use the same symbol, here \(
              f \), for all density functions, it helps to
              differentiate density functions by the parameters that
              they depend on.</span>

              \[ f(x \, | \, p) = p^x (1 - p) ^{1 - x}. \]
          </p>

          <p>
              You should verify that all three properties above are
              met with this density function \( f(x\,|\,p) \) defined for
              arbitrary \( p \in (0, 1) \).  If it helps, start with a
              specific value for \( p =
              \) <span class="probabilityp"></span>.
          </p>

          <p>
            <b>Example: (Discrete) Uniform Distribution</b>.  The
            density function associated with a discrete Uniform
            distribution can be defined on any finite collection of
            elements.  Each element is defined to have the same
            density.  Here's well define the density function over
            consecutive integers from \( a \) to \( b \), inclusive,
            such that \( S = \{a, a + 1, \ldots, b - 1, b \} \).
            
            \[ f(x \, | \, a, b) = \frac{1}{b + a - 1}. \]
          </p>

          <p>
            You should verify that all three properties above are met
            with this density function \( f(x \, | \, a, b) \) defined
            for arbitrary integers \(a, b \).  If it helps, start with
            specific values for \(a = \) <span class="unifa"></span>
            and \( b = \) <span class="unifb"></span>.
          </p>

          <p>
            <b>Example: Poisson Distribution</b>.  The density
            function associated with a Poisson distribution is defined
            on the natural numbers, \( S = \mathbb{N} \), and is used to
            model processes dealing with counts of arbitrary things
            over time and/or space.  The density function depends on a
            rate parameter \( \lambda \in \mathbb{R}^+ \) that
            dictates the rate at which the things are counted.  When
            \( \lambda \) is larger, more things are counted, on
            average, in each unit of time and/or space.
            
            \[ f(x \, | \, \lambda) = \frac{\exp{(-\lambda)} \lambda^x}{x!} \]
          </p>

          <p>
            You should verify that all three properties above are met
            with this density function \( f(x \, | \, \lambda ) \)
            defined for arbitrary rate parameter \( \lambda \).  If it
            helps, start with specific value for \( \lambda =
            \) <span class="poislambda"></span>.

          </p>

          <p>
            <b>Other Examples</b>. Binomial and Negative
            Binomial. TODO add links.
          </p>
      </section>

      <section>
          <h2 id="continous">Continuous Sample Spaces</h2>

          <p>
              When \( S \) is an uncountable
            space<span class="marginnote">We acknowledge that the
            expectation operator \( \mathbb{E} \) knows how to deal
            with different sizes of \( S \) associated with the
            respective density functions \( f \) with the subscript,
            \( \mathbb{E}_f \).</span>, say \( S \subset \mathbb{R}
            \), then the third property becomes

              \[ \mathbb{E}_f[\mathbb{I}_S] = \int_{\mathbb{R}} \mathbb{I}_S(x) f(x) dx = 1. \]
          </p>

          <p>
            Becasuse the integrand takes on the value \( f(x) \)
            whenever \(x \in S \) and \( 0 \) otherwise, this
            expectation is saying that the area under the density
            function integrates to \( 1 \) within the set \( S \).
            This is the same idea as with density functions on
            countable spaces summing to \( 1 \), now extended to
            uncountable spaces.
          </p>

          <p>
            <b>Example: Exponential
              Distribution</b>.<span class="marginnote">Follow these
              links TODO for more details on the family of Exponential
              Distributions, or a generalization of it, the family of
              Gamma Distributions.</span> Consider the family of
              Exponential Distributions, defined on \( S =
              \mathbb{R}^+ \), where the density function depends on
              the population parameter \( \lambda \in \mathbb{R}^+ \)
              and has the form

              \[ f(x | \lambda) = \lambda \exp{(-\lambda x)}. \]
          </p>


          <p>
              You should verify that all three properties above are
              met with the density function \( f(x | \lambda) \)
              defined for arbitrary \( \lambda \in \mathbb{R}^+ \).
              If it helps, start with a specific value for \( \lambda
              = \) <span class="explambda"></span>
          </p>

          <p>
            <b>Example: Normal Distribution.</b> The family of Normal
            distribution is the most widely used family of
            distributions.<span class="marginnote">Various TODO (add
            link to) Central Limit Theorems justify the popularity of
            the Normal distribution family.</span>  A Normal
            distribution has two parameters, the location parameter \(
            \mu \in \mathbb{R} \) and a scale parameter \( \sigma \in
            \mathbb{R}^+ \).  The density function associated with the Normal distribution is defined on \( S = \mathbb{R} \) and has the form

            \[ f(x \, | \, \mu, \sigma) = (2 \pi \sigma^2)^{-1/2} \exp{\left( (2\sigma^2)^{-1} (x - \mu)^2 \right)}. \]
          </p>

          <p>
            <b>Other Examples.</b> Gamma, Rayleigh, Lognormal. TODO add links.
          </p>
      </section>

      <section>
        <h2 id="uses">Uses</h2>

        <p>
          Density functions are often the means through which applied
          statisticians model the world around them.  When the process
          of interest has only two outcomes, generally speaking, a
          Bernoulli distribution is recommended.  When counts are
          involved a Poisson distribution a reasonable first step.
          When the process of interest can only take on positive
          numbers, a Gamma distribution is often, but not exclusively,
          used.  And when all one cares about is estimating the mean,
          then the Normal distribution is always a favorite by
          application of the Central Limit Theorem.  In an attempt to
          generalize this take away, since the world is often not as
          simple as repeated if-else statements, think about the
          quantification of the process you are interested in.  Then
          imagine, what's the likely shape that repeated and noisy
          observations of this process are likely to take on.  By
          imagining what noisy repeated observations of your process
          might look like and what values they could realistically
          take on, you should be able to trace out at least a sketch
          of a density function that is suitable to your problem.
        </p>

        <p>
          Once the applied statistician has the necessary density
          functions in mind, they are used in, broadly speaking, one
          of two methods to estimate the parameters of interest.  The
          two methods are Bayesian statistics or the likelihood
          method.  Very brief outlines of these methods are provided
          below, with links to more detailed discussions.
        </p>

        <p>
          <b>Example: Bayesian
          Statistics</b>.<span class="marginnote">See the lecture
          notes on <a href="lecturenotes/bayesian.html">Bayesian
          Statistics</a> for more details.</span>  For not terribly
          difficult reasons, albeit off topic here, Bayesian
          statistics defines a posterior density funcition, which
          we'll call \( \pi \).  This posterior density function
          describes the updated information we have about the unkown
          parameters \( \theta \in \mathbb{R}^D \) after learning
          about them from a vector of observations \( \mathbf{x} \)
          via the model contained in two other density functions.  The
          density function \( f(\mathbf{x} \, | \, \theta) \) is often
          called the likelihood or the model of our data \( \mathbf{x}
          \) dependent on some unknown parameters \( \theta \), and
          the density function \( p(\theta) \) is used to describe
          background knowledge about the parameters that may or may
          not be included in the data \( \mathbf{x} \).  The defintion
          of the posterior density function is

          \[ \pi(\theta \, | \, \mathbf{x}) = \frac{f( \mathbf{x} \, | \, \theta) p(\theta)} {\mathbb{E}_{p} [f]}. \]
        </p>

        <p>
          Since the definition of the posterior density function is of
          central importantce to Bayesian statistics, and it depends
          on other density functions, this example highlights the
          importance of density functions.
        </p>

        <p>
          <b>Example: Likelihood</b>.<span class="marginnote"> See the
          lecture notes on
          the <a href="lecturenotes/likelihood.html">likelihood
          method</a> for more details.</span> The likelihood method is
          used to estimate population parameters \( \theta \in
          \mathbb{R}^D \) given a vector of data \( \mathbf{x} \) via
          the density function \( f( x \, | \, \theta) \).
          Esimation is accomplished by use of the likelihood function.
          The likelihood function is defined as a product of density
          functions over the (assumed) independent observations \(
          \mathbf{x} \)

          \[ L(\theta \, | \, \mathbf{x}) = \prod_{n=1}^N f(x_n \, | \, \theta). \]
        </p>

        <p>
          Since the likelihood method is the
          other<span class="marginnote">I'm ignoring a few other
          methods that, in my experience, seem less common, namely
          method of moments, U statistics, and fiducial statistics.
          Although, density functions show up in these methods as
          well.</span>, different from Bayesian statistics, main
          method used to learn about population parameters from data,
          we see again that density functions play a key role in the
          world of statistics.
        </p>
      </section>

      
      <hr>
      <a href="https://creativecommons.org/licenses/by-nc-sa/4.0">
        Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
      </a>
    </article>

    <script type="text/javascript">
      function probability() {
          let p = Math.floor(100 * Math.random()) / 100,
              explambda = Math.floor(100 * Math.random()) / 10,
              poislambda = Math.floor(100 * Math.random()) / 10,
              unifa = Math.floor(10 * Math.random()),
              unifb = Math.floor(100 * Math.random());
          [unifa, unifb] = unifb < unifa ? [unifb - 1, unifa + 1] : [unifa, unifb];

          d3.selectAll(".probabilityp")
              .attr("style", "border-bottom: 1px dotted #000;")
              .text(p)
              .call(d3.drag()
                    .on("drag", adjustp));

          d3.selectAll(".explambda")
              .attr("style", "border-bottom: 1px dotted #000;")
              .text(explambda)
              .call(d3.drag()
                    .on("drag", adjustExpLambda));

          d3.selectAll(".poislambda")
              .attr("style", "border-bottom: 1px dotted #000;")
              .text(poislambda)
              .call(d3.drag()
                    .on("drag", adjustPoisLambda));

          d3.selectAll(".unifa")
              .attr("style", "border-bottom: 1px dotted #000;")
              .text(unifa)
              .call(d3.drag()
                    .on("drag", adjustUnifA));

          d3.selectAll(".unifb")
              .attr("style", "border-bottom: 1px dotted #000;")
              .text(unifb)
              .call(d3.drag()
                    .on("drag", adjustUnifB));

          function adjustp() {
              p += d3.event.dx / 100;
              p = p < 0 ? 0 : p;
              p = p > 1 ? 1 : p;

              p = Math.round(p * 100) / 100;
              d3.selectAll(".probabilityp")
                  .text(p);
          }
          
          function adjustExpLambda() {
              explambda += d3.event.dx / 25;
              explambda =  explambda < 0 ? 0.01 : explambda;
              explambda = explambda > 10 ? 10 : explambda;

              explambda = Math.round(explambda * 100) / 100;
              d3.selectAll(".explambda")
                  .text(explambda);
          }

          function adjustPoisLambda() {
              poislambda += d3.event.dx / 25;
              poislambda =  poislambda < 0 ? 0.01 : poislambda;
              poislambda = poislambda > 10 ? 10 : poislambda;

              poislambda = Math.round(poislambda * 100) / 100;
              d3.selectAll(".poislambda")
                  .text(poislambda);
          }

          function adjustUnifA() {
              unifa += d3.event.dx / 25;
              unifa =  unifa < 0 ? 0 : unifa;
              unifa = unifa > unifb ? unifb : unifa;

              unifa = Math.round(unifa);
              d3.selectAll(".unifa")
                  .text(unifa);
          }

          function adjustUnifB() {
              unifb += d3.event.dx / 10;
              unifb =  unifb < unifa ? unifa + 1 : unifb;
              unifb = unifb > 100 ? 100 : unifb;

              unifb = Math.round(unifb);
              d3.selectAll(".unifb")
                  .text(unifb);
          }
      };
      probability();
    </script>
  </body>
</html>
